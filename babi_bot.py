# -*- coding: utf-8 -*-
"""babi-bot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mSrwbqUGzllkuhBvnJOXVa0YKKwgRqvs
"""

from google.colab import drive
drive.mount('/gdrive')

from google.colab import files
files.upload()

files.upload()

import pickle 
import numpy as np

with open("train_qa.txt", "rb") as f:
    train_data = pickle.load(f)

with open("test_qa.txt", "rb") as f:
    test_data = pickle.load(f)

type(test_data)
type(train_data)
len(test_data)
len(train_data)

train_data[0]

' '.join(train_data[0][0])

' '.join(train_data[0][1])

train_data[0][2]

# Set up the vocabulary

vocab = set() # this will hold the vocab

all_data = test_data + train_data

for story, question, answer in all_data:
    vocab = vocab.union(set(story))
    vocab = vocab.union(set(question))

vocab.add('no')
vocab.add('yes')

vocab

vocab_len = len(vocab) + 1 # add an extra space to hold a 0 for Keras' pad sequences

max_story_len = max([len(data[0]) for data in all_data])

max_story_len

max_question_len = max([len(data[1]) for data in all_data])

max_question_len

"""### Vectorize Data"""

vocab

vocab_size = len(vocab) + 1

"""### Import Keras Packages"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install keras

from keras.preprocessing.sequence import pad_sequences 
from keras.preprocessing.text import Tokenizer

# int-encode sequences of words
tokenizer = Tokenizer(filters=[])
tokenizer.fit_on_texts(vocab)

tokenizer.word_index

train_story_text = []
train_question_text = []
train_answers = []

for story, question, answer in train_data:

    train_story_text.append(story)
    train_question_text.append(question)

train_story_seq = tokenizer.texts_to_sequences(train_story_text)

print(len(train_story_text))
len(train_story_seq)

"""### Functionalize Vectorization Process"""

def vectorize_stories(data, 
                      word_index=tokenizer.word_index, 
                      max_story_len=max_story_len, 
                      max_question_len=max_question_len):
    '''
    INPUT:

    data: consisting of Stories, Queries, and Answers
    word_index: word index dictionary from tokenizer
    max_story_len: the length of the longest story ( used for pad_sequences function )
    max_question_len: length of the longest question ( used for pad_sequences function )

    OUTPUT:

        Vectorizes the stories, questions, and answers into padded sequences. 
        First loop for every story,query,answer in the data. Then convert raw words
        to a word index value. Then append each set to their appropriate output list.
        Once converted --> words to numbers, pad the sequences so they are all of equal length.

    Returns: above output explanation in the form of a tuple (X,Xq,Y) (padded based on max lengths)
    '''
    # X = Stories
    X = []
    # Xq = Query
    Xq = []
    # Y = Correct Answer
    Y = []

    for story, query, answer in data:
        
        # Grab word index for every word in the story
        x = [word_index[word.lower()] for word in story]
        # Grab the word index for every word in query
        xq = [word_index[word.lower()] for word in query]

        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)
        # INdex 0 is reserved so we're going to use + 1
        y = np.zeros(len(word_index) + 1)

        # Now that y is all zeros and it's known it is just Yes/No
        ## We can use numpy logic to create this assignment
        y[word_index[answer]] = 1

        # Append each set of story,query,answer to their respective holding lists
        X.append(x)
        Xq.append(xq)
        Y.append(y)

    # Finally, pad the sequences based on their max length so the RNN can be trained
    ## on uniformly long sequences. RETURN TUPLE FOR UNPACKING!!!
    return (pad_sequences(X, maxlen=max_story_len), 
            pad_sequences(Xq, maxlen=max_question_len), 
            np.array(Y))

inputs_train, queries_train, answers_train = vectorize_stories(train_data)
inputs_test, queries_test, answers_test = vectorize_stories(test_data)

print(inputs_test)
queries_test

print(answers_test)
print(sum(answers_test))
print(tokenizer.word_index['yes'])
tokenizer.word_index['no']

"""## Creating the Model"""

# IMPORT RELATIVE PACKAGES
from keras.models import Sequential, Model 
from keras.layers.embeddings import Embedding
from keras.layers import Input, Activation, Dense, Permute, Dropout 
from keras.layers import add, dot, concatenate 
from keras.layers import LSTM

"""### Placeholders for Inputs

Recall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.

"""

input_sequence = Input((max_story_len,))
question = Input((max_question_len,))

"""## Encoders

### Input Encoder m
"""

# Input gets embedded to a sequence of vectors
input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))
input_encoder_m.add(Dropout(0.3))

"""### Input Encoder c"""

# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=max_question_len))
input_encoder_c.add(Dropout(0.3))

"""### Question Encoder"""

# embed the question into a sequence of vectors
question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=64,
                               input_length=max_question_len))
question_encoder.add(Dropout(0.3))

"""### Encode the Sequences"""

# encode input sequence and questions ( which are indicies )
# to sequences of dense vectors
input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)

# USE DOT PRODUCT TO COMPUTE THE MATCH BETWEEN FIRST INPUT VECTOR AND QUERY
match = dot([input_encoded_m, question_encoded], axes=(2, 2))
match = Activation("softmax")(match)

# Add match matrix with the second input vector sequence
response = add([match, input_encoded_c])
response = Permute((2, 1))(response)

# CONCATENATE
answer = concatenate([response, question_encoded])

answer

# Reduce with RNN (LSTM)
answer = LSTM(32)(answer)

answer = Dropout(0.5)(answer)
answer = Dense(vocab_size)(answer)

answer = Activation("softmax")(answer)

# build final model 
model = Model([input_sequence, question], answer)
model.compile(optimizer="rmsprop", 
              loss="categorical_crossentropy",
              metrics=["accuracy"])

model.summary()

# TRAIN
history = model.fit([inputs_train, queries_train],
                    answers_train,
                    batch_size=32,
                    epochs=120,
                    validation_data=([inputs_test, queries_test],
                                     answers_test))

"""### Save Model"""

filename = "chatteral_thinking_120_epochs.h5"
model.save(filename)

"""## Evaluating Model

### Plotting Training History
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

print(history.history.keys())

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show();

"""### Evaluation on Given Test Set"""

model.load_weights(filename)
pred_results = model.predict(([inputs_test, queries_test]))

test_data[0][0]

story = ' '.join(word for word in test_data[0][0])
print(story)

query = ' '.join(word for word in test_data[0][1])
print(query)

print("True Test Answer from Data is: ", test_data[0][2])

# Generate prediction from model
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])

## test run

vocab

my_story = "John left the kitchen . Sandra dropped the football in the garden ."
my_story.split()

my_question = "Is the football in the garden ?"

my_question.split()

mydata = [(my_story.split(), my_question.split(), 'yes')]

my_story, my_ques,my_ans = vectorize_stories(mydata)

pred_results = model.predict(([my_story, my_ques]))

#### GENERATE FINAL PREDICTION FROM MODEL
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key 

print(f"Predicted answer is: {k}")
print(f"Probability of Certainty was: {pred_results[0][val_max]}")

